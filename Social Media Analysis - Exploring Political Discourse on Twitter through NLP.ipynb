{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77dabbe8",
   "metadata": {},
   "source": [
    "### Social Media Analysis: Exploring Political Discourse on Twitter through NLP\n",
    "#### Meier Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01ac50",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project delves into social media analysis focusing on political discourse, particularly on Twitter. Utilizing Natural Language Processing (NLP) techniques, we aim to conduct topic modeling and sentiment analysis on tweets related to political content.\n",
    "\n",
    "The dataset used in this project is from [Kaggle: Joe Biden Tweets (2007 - 2020)](https://www.kaggle.com/datasets/rohanrao/joe-biden-tweets?select=JoeBidenTweets.csv). It contains 6062 Joe Biden's tweets posted from 24th October 2007 to 31st October 2020.\n",
    "\n",
    "There are six parts in this project, which are introduction, preparation, data preprocessing, descriptive statistics, topic modeling and sentiment analysis.\n",
    "\n",
    "__Preparaction__\n",
    "\n",
    "In this section, the packages and data are imported. And I would sample 100 tweets posted during the campaign for this project.\n",
    "\n",
    "__Data Preprocessing__\n",
    "\n",
    "The data preprocess includes (1) removing URLs and HTML tags, (2) removing punctuations, (3) removing stopwords, (4) lowering case, (5) lemmatization and (5) tokenization.\n",
    "\n",
    "__Descriptive Statistics__\n",
    "\n",
    "To explore the fundamental characteristics of the data, a descriptive analysis will be done in this section.\n",
    "\n",
    "__Topic Modeling__\n",
    "\n",
    "The aim of this project is to analyze the patterns of Biden's tweets for the 2020 presidential campaign. Therefore, I would apply LDA algorithm as it has shown excellent results in practice and visualize the result by pyLDAvis package.\n",
    "\n",
    "__Sentiment Analysis__\n",
    "\n",
    "To explore further, sentiment analysis will be done by using Hugging face transformers. I will display the top 10 common words by different sentiments.\n",
    "\n",
    "\n",
    "__Reference__\n",
    "\n",
    "* Kedia, A. (2020). Hands-on python natural language processing: Explore tools and techniques to analyze and process text with a view to building real-world nlp applications. Packt Publishing Limited.\n",
    "* PÃ©rez, J. M., Giudici, J. C., & Luque, F. (2021). pysentimiento: A python toolkit for sentiment analysis and socialnlp tasks. arXiv preprint arXiv:2106.09462."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efecbd",
   "metadata": {},
   "source": [
    "### Preparaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from transformers import pipeline\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2aae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"JoeBidenTweets.csv\")\n",
    "\n",
    "# remove missing and duplicate values\n",
    "df.dropna(axis = \"columns\", inplace = True)\n",
    "df.drop_duplicates(inplace = True, subset = \"tweet\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb30e19",
   "metadata": {},
   "source": [
    "My aim is to explore the patterns of Biden's tweets for the 2020 presidential campaign. Therefore, I would extract the tweets posted after 25th of April, 2019, which is the date that Biden officially posted a video and claimed to join the campaign. Firstly, I convert the timestamp column to DateTime format. Then, I check the time of the last tweet in this dataset and the amount of tweets posted during the campaign to ensure I can get enough data for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column timestamp to DateTime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# the last tweet\n",
    "df.sort_values('timestamp', ascending = True, inplace = True)\n",
    "last_tweet = df.timestamp.iloc[-1]\n",
    "print(last_tweet)\n",
    "\n",
    "# the amount of tweet posted during the campaign\n",
    "df_cam = df.loc[df['timestamp'] >= '2019-04-25']\n",
    "len(df_cam.tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488c68c",
   "metadata": {},
   "source": [
    "The last tweet was posted on 2020.11.01, and 4722 tweets were posted during the campaign. For this project, I would sample 100 tweets randomly for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100 tweets\n",
    "sample_df = df_cam.sample(n = 100, ignore_index=True, random_state = 100)\n",
    "tweets = sample_df[['tweet']]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b12d89",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look of the data\n",
    "with pd.option_context('display.max_rows',5, 'display.max_colwidth', None): \n",
    "    display(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb16e2d",
   "metadata": {},
   "source": [
    "As it is shown above, the tweets contain URL links and HTML parsers. Therefore, I clean them first and save the result for sentiment analysis. After that, I preprocess the data by removing punctuations, lowering case, removing stopwords and lemmatization. I keep the @ and # as I also want to look at the users and hashtags in the tweets. Finally, I tokenize the tweets into bigrams for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfe135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove URLs and ampersand by HTML parsers (&amp)\n",
    "def remove_links(tweet):\n",
    "        tweet = re.sub(r'http[^\\s]+','',str(tweet))\n",
    "        tweet = re.sub('&amp','',str(tweet))\n",
    "        return tweet\n",
    "    \n",
    "tweets['cleaned_tweets'] = tweets['tweet'].apply(remove_links)\n",
    "tweets['cleaned_tweets_sa'] = tweets['cleaned_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.Series(tweets.cleaned_tweets.tolist()).astype(str)\n",
    "\n",
    "# remove punctuations (expect for @ and #) and lowering case \n",
    "def text_clean(corpus, keep_list):\n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9@#]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def stopwords_removal(corpus):\n",
    "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70655bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, lemmatization = True, remove_stopwords = True):\n",
    " \n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        corpus = stopwords_removal(corpus)\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        corpus = lemmatize(corpus)\n",
    "        \n",
    "        \n",
    "    corpus = [' '.join(x) for x in corpus]        \n",
    "\n",
    "    return corpus\n",
    "\n",
    "keep_list = ['U.S.A', 'U.S.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = preprocess(corpus, keep_list, cleaning = True, lemmatization = True, remove_stopwords = True)\n",
    "tweets['cleaned_tweets'] = pd.Series(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization: bigrams\n",
    "def generate_bigrams(text,ngram=2):\n",
    "    \n",
    "    words=[word for word in text.split(\" \") if word not in set(stopwords.words('english'))]  \n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "\n",
    "    return ans\n",
    "\n",
    "tweets['bigrams'] = tweets['cleaned_tweets'].apply(generate_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aa70b",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f2789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word count\n",
    "def word_count(text):\n",
    "   \n",
    "    return len(text.split())\n",
    "\n",
    "tweets['original_tweet_word_count'] = tweets['tweet'].apply(word_count)\n",
    "tweets['cleaned_tweet_word_count'] = tweets['cleaned_tweets'].apply(word_count)\n",
    "tweets['bigram_count'] = tweets['bigrams'].str.len()\n",
    "\n",
    "tweets.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb28a0",
   "metadata": {},
   "source": [
    "The table above shows the word count for original tweets, tweets after preprocessing and bigrams. To save space, only the first three tweets are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bigrams - all sample data\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]\n",
    "\n",
    "top_10_bigrams = get_top_ngram(tweets['cleaned_tweets'],2)[:10] \n",
    "x,y = map(list,zip(*top_10_bigrams)) \n",
    "sns.barplot(x = y,y = x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1f3fc",
   "metadata": {},
   "source": [
    "The bar chart shows the top 10 bigrams in the sample data. From this chart, we can tell Biden mentioned president Donald Trump a lot which quite makes sense as they are competitors. Following, I will explore further on Biden's tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f6fda",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f072240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model by bigrams\n",
    "id2word = Dictionary(tweets['bigrams'])\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_bigrams = [id2word.doc2bow(text) for text in tweets['bigrams']]\n",
    "\n",
    "[[(id2word[i], freq) for i, freq in doc] for doc in corpus_bigrams[:1]]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model_bigrams = LdaModel(corpus=corpus_bigrams,\n",
    "                   id2word=id2word,\n",
    "                   num_topics=2, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)\n",
    "\n",
    "coherence_model_lda_bigrams = CoherenceModel(model=lda_model_bigrams, texts=tweets['bigrams'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda_bigrams.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df7733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display LDA model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model_bigrams, corpus_bigrams, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9856c37",
   "metadata": {},
   "source": [
    "Both topics are relevant to Donald Trump, but we can still find some differences. Topic 1 seems to mention America from the perspective of a nation ('american people', 'american history' are mentioned a lot), which brings connections from culture or history. While topic 2 is more close to the perspective of the state (includes 'covid 19', 'government work'), which means characterising the country by formal institutions or policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd25a8",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a43002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transformers\n",
    "classifier = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "sentiment_analysis = tweets.cleaned_tweets_sa.apply(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be72ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save sentiment analysis result\n",
    "labels=[]\n",
    "scores=[]\n",
    "for sentiment in sentiment_analysis:\n",
    "    labels.append(sentiment[0]['label'])\n",
    "    scores.append(round(sentiment[0]['score'],4))\n",
    "    \n",
    "tweets['labels'] = labels\n",
    "tweets['scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis result \n",
    "print(tweets.groupby(['labels'])['labels'].count())\n",
    "\n",
    "neg_tweets = tweets[tweets['labels'] == 'NEG']\n",
    "neu_tweets = tweets[tweets['labels'] == 'NEU']\n",
    "pos_tweets = tweets[tweets['labels'] == 'POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bigrams - Negative\n",
    "top_10_neg = get_top_ngram(neg_tweets['cleaned_tweets'],2)[:10] \n",
    "x1,y1 = map(list,zip(*top_10_neg)) \n",
    "sns.barplot(x = y1,y = x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd782cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bigrams - Neutral\n",
    "top_10_neu = get_top_ngram(neu_tweets['cleaned_tweets'],2)[:10] \n",
    "x2,y2 = map(list,zip(*top_10_neu)) \n",
    "sns.barplot(x = y2,y = x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bigrams - Positive\n",
    "top_10_pos = get_top_ngram(pos_tweets['cleaned_tweets'],2)[:10] \n",
    "x3,y3 = map(list,zip(*top_10_pos)) \n",
    "sns.barplot(x = y3,y = x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72984067",
   "metadata": {},
   "source": [
    "We can see 'donald trump' remains a high frequency in all the sentiments, but it is not the most frequent bigrams in positive sentiment. The tweets labelled as negative seem to describe Trump's term of office (as it shows 'supreme court' 'repeal aca'), while the positive tweets are more close to Biden himself (as it shows 'vice president'). And for the neutral tweets, public topics such as covid-19 are mentioned.\n",
    "\n",
    "To sum up, this project presents a descriptive and exploratory analysis for detecting Biden's campaign strategies on social media. More theoretical evidence on social media and political campaigns is needed to explain the outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
